{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKyC3mZ72qlH"
      },
      "source": [
        "Copy-paste the contents of this notebook at the beginning of every other notebook in this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBe5K-JpHXFs"
      },
      "source": [
        "## Variable hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "httbkgCwHYF4"
      },
      "outputs": [],
      "source": [
        "# Fixed length of list to be sorted\n",
        "LIST_LENGTH = 5\n",
        "\n",
        "# Size of vocabulary\n",
        "D_VOCAB = 66\n",
        "\n",
        "# Should lists have repetitions?\n",
        "ALLOW_REPETITIONS = False\n",
        "\n",
        "# Attention only? (False -> model includes MLPs)\n",
        "ATTN_ONLY = True\n",
        "\n",
        "# Model dimenions\n",
        "N_LAYERS = 1\n",
        "N_HEADS = 1\n",
        "D_MODEL = 128\n",
        "D_HEAD = 32\n",
        "D_MLP = None\n",
        "\n",
        "if ATTN_ONLY:\n",
        "    D_MLP = None\n",
        "\n",
        "# Default batch size\n",
        "DEFAULT_BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o--LiD_HMGs"
      },
      "source": [
        "## Prelude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjVSLZbEEJX"
      },
      "source": [
        "### Install and import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqa5uVY-2oC7"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import transformer_lens\n",
        "except:\n",
        "    !pip install git+https://github.com/neelnanda-io/TransformerLens\n",
        "    !pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "dqFfW5V32yaO",
        "outputId": "323df961-4b90-49e8-8ab8-ef66c85061d1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-6a46df50-5541\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.38.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-6a46df50-5541\",\n",
              "      Hello,\n",
              "      {\"name\": \"You\"}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f66f6215730>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime as dt\n",
        "from itertools import repeat\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from typing import cast, Generator, Literal\n",
        "\n",
        "import circuitsvis as cv\n",
        "from fancy_einsum import einsum\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, tensor, Tensor, TensorType as TT\n",
        "from torch.nn import functional as F\n",
        "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
        "from tqdm import tqdm\n",
        "from typing_extensions import Self\n",
        "\n",
        "cv.examples.hello(\"You\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo4oP4dSrYhD"
      },
      "source": [
        "### Invariable hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XwivtyM_Hru",
        "outputId": "253931dc-5c4d-4f11-a4e9-b80730e9372c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE = 'cpu'\n"
          ]
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"{DEVICE = }\")\n",
        "\n",
        "# Seeds to generate training, validation, and test data\n",
        "TRAIN_SEED = 42\n",
        "VAL_SEED = 66\n",
        "TEST_SEED = 1729\n",
        "\n",
        "# Context length: [start, *(unsorted_)list_length, mid, *(sorted_)list_length]\n",
        "N_CTX = 2 * LIST_LENGTH + 2\n",
        "\n",
        "# \"Real\" tokens range from 0 to D_VOCAB - 2 (non-inclusive)\n",
        "VOCAB_MIN_ID = 0\n",
        "VOCAB_MAX_ID = D_VOCAB - 2\n",
        "\n",
        "# START token is D_VOCAB - 2 and MID token is D_VOCAB - 1\n",
        "START_TOKEN_ID = VOCAB_MAX_ID\n",
        "MID_TOKEN_ID = D_VOCAB - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlWGwox63j24"
      },
      "source": [
        "### Data generator and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44OZjOq83k1H",
        "outputId": "413c82ad-aa84-42bc-f9ab-fe4c0608481a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_data.shape=torch.Size([1000, 12]); test_data.shape=torch.Size([1000, 12])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([22, 43, 20,  0,  0, 51, 28, 33, 28, 52, 42, 13]),\n",
              " tensor([64, 37, 63, 51, 58,  3, 65,  3, 37, 51, 58, 63]))"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def generate_list(batch_size: int) -> Tensor:\n",
        "    if ALLOW_REPETITIONS:\n",
        "        return torch.randint(VOCAB_MIN_ID, VOCAB_MAX_ID, (batch_size, LIST_LENGTH)).to(DEVICE)\n",
        "    return tensor([\n",
        "        random.sample(range(VOCAB_MIN_ID, VOCAB_MAX_ID), k=LIST_LENGTH) \n",
        "        for _ in range(batch_size)\n",
        "    ]).to(DEVICE)\n",
        "\n",
        "# General generator\n",
        "def make_data_gen(\n",
        "    *,\n",
        "    batch_size: int = DEFAULT_BATCH_SIZE,\n",
        "    dataset: Literal[\"train\", \"val\", \"test\"], # probably this arg needs a better name,\n",
        ") -> Generator[Tensor, None, None]:\n",
        "    assert dataset in (\"train\", \"val\", \"test\")\n",
        "    if dataset == \"train\":\n",
        "        seed = TRAIN_SEED\n",
        "    elif dataset == \"val\":\n",
        "        seed = VAL_SEED\n",
        "    else: # test\n",
        "        seed = TEST_SEED\n",
        "    torch.manual_seed(seed)\n",
        "    while True:\n",
        "        # Generate random numbers\n",
        "        x = generate_list(batch_size)\n",
        "        # Sort\n",
        "        x_sorted = torch.sort(x, dim=1).values\n",
        "        # START tokens\n",
        "        x_start = START_TOKEN_ID * torch.ones(batch_size, dtype=torch.int32).reshape(batch_size, -1).to(DEVICE)\n",
        "        # MID tokens\n",
        "        x_mid = MID_TOKEN_ID * torch.ones(batch_size, dtype=torch.int32).reshape(batch_size, -1).to(DEVICE)\n",
        "        yield torch.cat((x_start, x, x_mid, x_sorted), dim=1)\n",
        "\n",
        "\n",
        "# Training data generator (kinda wrapper)\n",
        "def make_train_gen() -> Generator[Tensor, None, None]:\n",
        "    \"\"\"Make generator of training data\"\"\"\n",
        "    return make_data_gen(batch_size=128, dataset=\"train\")\n",
        "\n",
        "# Validation and test data\n",
        "\n",
        "val_data = next(make_data_gen(batch_size=1000, dataset=\"val\"))\n",
        "test_data = next(make_data_gen(batch_size=1000, dataset=\"test\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBJn1fKm3pMX"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTlNfaKY3qkd",
        "outputId": "7d962ac8-efe5-4681-bb20-a4ff8b675156"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.5333, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def loss_fn(\n",
        "    logits: Tensor, # [batch, pos, d_vocab] \n",
        "    tokens: Tensor, # [batch, pos] \n",
        "    return_per_token: bool = False\n",
        ") -> Tensor: # scalar\n",
        "    \"\"\"\"\"\"\n",
        "    # \n",
        "    sorted_start_pos = LIST_LENGTH + 2\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    tokens = tokens[:, sorted_start_pos : None]\n",
        "    log_probs = logits.log_softmax(-1)\n",
        "    correct_log_probs = log_probs.gather(-1, tokens[..., None])[..., 0]\n",
        "    if return_per_token:\n",
        "        return -correct_log_probs\n",
        "    return -correct_log_probs.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKKrzLEq3sun"
      },
      "source": [
        "### Accuracy and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x6q7b7O3rZa"
      },
      "outputs": [],
      "source": [
        "def get_diff_row_inds(\n",
        "    a: Tensor, # [dim1, dim2]\n",
        "    b: Tensor  # [dim1, dim2]\n",
        ") -> Tensor:   # [dim1]\n",
        "    \"\"\"Find indices of rows where a and b differ\"\"\"\n",
        "    assert a.shape == b.shape\n",
        "    return ((a == b).prod(dim=1) == 0).nonzero(as_tuple=True)[0]\n",
        "\n",
        "def acc_fn(\n",
        "    logits: Tensor, # [batch, pos, d_vocab]\n",
        "    tokens: Tensor, # [batch, pos]\n",
        "    per: Literal[\"token\", \"sequence\"] = \"sequence\"\n",
        ") -> float:\n",
        "    \"\"\"Compute accuracy as percentage of correct predictions\"\"\"\n",
        "    assert per in (\"token\", \"sequence\")\n",
        "    sorted_start_pos = LIST_LENGTH + 2\n",
        "    # Get logits of predictions for position\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    preds = logits.argmax(-1)\n",
        "    tokens = tokens[:, sorted_start_pos:]\n",
        "    if per == \"sequence\":\n",
        "        return (preds == tokens).prod(dim=1).float().mean().item()\n",
        "    return (preds == tokens).float().mean().item()\n",
        "\n",
        "def validate(\n",
        "    model: HookedTransformer, \n",
        "    data: Tensor, # [batch, pos]\n",
        "    per: Literal[\"token\", \"sequence\"] = \"sequence\"\n",
        ") -> float:\n",
        "    \"\"\"Test this model on `data`\"\"\"\n",
        "    logits = model(data)\n",
        "    acc = acc_fn(logits, tokens=data, per=per)\n",
        "    return acc\n",
        "\n",
        "def show_mispreds(\n",
        "    model: HookedTransformer, \n",
        "    data: Tensor # [batch, pos]\n",
        ") -> None:\n",
        "    \"\"\"Test this model on `data` and print mispredictions\"\"\"\n",
        "    logits = model(data)\n",
        "    sorted_start_pos = LIST_LENGTH + 2\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    tokens = data[:, sorted_start_pos:].cpu()\n",
        "    preds = logits.argmax(-1).cpu()\n",
        "    mispred_inds = get_diff_row_inds(tokens, preds)\n",
        "    for i in mispred_inds:\n",
        "        print(f\"[{i}] {tokens[i].numpy().tolist()} | {preds[i].numpy().tolist()}\")\n",
        "    print(f\"{len(mispred_inds)}/{len(preds)} ({len(mispred_inds) / len(preds) :.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5trW5nKhF3I"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Hj7N1nhHrI"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwIffvImhT4g"
      },
      "outputs": [],
      "source": [
        "cfg = HookedTransformerConfig(\n",
        "    d_model=D_MODEL,\n",
        "    n_layers=N_LAYERS,\n",
        "    n_heads=N_HEADS,\n",
        "    d_head=D_HEAD,\n",
        "    n_ctx=N_CTX,\n",
        "    d_vocab=D_VOCAB,\n",
        "    act_fn=\"relu\",\n",
        "    seed=42,\n",
        "    device=DEVICE,\n",
        "    attn_only=ATTN_ONLY\n",
        ")\n",
        "model = HookedTransformer(cfg, move_to_device=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vYV7QAd3vCT"
      },
      "source": [
        "### Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHBPL3Xw3uAt"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TrainingHistory:\n",
        "    losses: list[float]\n",
        "    train_accuracies: list[float]\n",
        "    val_accuracies: list[float]\n",
        "\n",
        "def converged(val_accs: list[float], n_last: int = 10) -> bool:\n",
        "    return cast(bool, (tensor(val_accs[-n_last:]) == 1).all().item())\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 20000\n",
        "\n",
        "# Optimization\n",
        "lr = 1e-3\n",
        "betas = (.9, .999)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \"min\", patience=100)\n",
        "\n",
        "# Training data generator\n",
        "train_gen = make_train_gen()\n",
        "\n",
        "def train_model(model: HookedTransformer) -> TrainingHistory:\n",
        "    losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        tokens = next(train_gen).to(device=DEVICE)\n",
        "        logits = model(tokens)\n",
        "        loss = loss_fn(logits, tokens)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        scheduler.step(loss)\n",
        "        \n",
        "        if epoch % 100 == 0:\n",
        "            losses.append(loss.item())\n",
        "            train_batch_acc = acc_fn(logits, tokens)\n",
        "            val_acc = validate(model, val_data)\n",
        "            val_loss = loss_fn(model(val_data), val_data)\n",
        "\n",
        "            train_accuracies.append(train_batch_acc)\n",
        "            val_accuracies.append(val_acc)\n",
        "            print(\n",
        "                f\"Epoch {epoch}/{n_epochs} ({epoch / n_epochs:.0%}) : \"\n",
        "                f\"loss = {loss.item():.4f}; {train_batch_acc=:.3%}; \"\n",
        "                f\"{val_acc=:.3%}; lr={scheduler._last_lr[0]}\" #type:ignore\n",
        "            )\n",
        "            # If last 10 recorded val_accuracies are 100%\n",
        "            if converged(val_accuracies):\n",
        "                print(f\"\\nAchieved consistent perfect validation accuracy after {epoch} epochs\")\n",
        "                break\n",
        "    return TrainingHistory(losses, train_accuracies, val_accuracies)\n",
        "\n",
        "def load_model_state(model: HookedTransformer, filename: str) -> None:\n",
        "    assert os.path.isdir(\"models\"), \"Make a directory `models` with model state dicts\"\n",
        "    if not filename.startswith(\"models/\"):\n",
        "        filename = f\"models/{filename}\"\n",
        "    with open(filename, \"rb\") as f:\n",
        "        state_dict = pickle.load(f)\n",
        "    model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXhR2JBjdXkD"
      },
      "source": [
        "### Train or load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMH8DwNHdWLA"
      },
      "outputs": [],
      "source": [
        "# history = train_model(model)\n",
        "# load_model_state(model, <filename>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HABWttcH34dv"
      },
      "source": [
        "### Testing post-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lakshPyuwdO7",
        "outputId": "259e2ca7-0398-476f-f6ba-094006179a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating on validation data:\n",
            "\tval_acc=99.900%\n",
            "\n",
            "[365] [3, 48, 49, 56, 57] | [3, 49, 49, 56, 57]\n",
            "1/1000 (0.10%)\n",
            "\n",
            "Validating on test data:\n",
            "\ttest_acc=99.700%\n",
            "\n",
            "[110] [5, 7, 9, 57, 61] | [5, 7, 9, 61, 61]\n",
            "[371] [13, 19, 60, 61, 63] | [13, 19, 19, 61, 63]\n",
            "[633] [29, 30, 57, 58, 63] | [29, 30, 30, 58, 63]\n",
            "3/1000 (0.30%)\n"
          ]
        }
      ],
      "source": [
        "print(\"Validating on validation data:\")\n",
        "val_acc = validate(model, val_data)\n",
        "print(f\"\\t{val_acc=:.3%}\\n\")\n",
        "if val_acc < 1:\n",
        "    show_mispreds(model, val_data)\n",
        "\n",
        "print(\"\\nValidating on test data:\")\n",
        "test_acc = validate(model, test_data)\n",
        "print(f\"\\t{test_acc=:.3%}\\n\")\n",
        "if test_acc < 1:\n",
        "    show_mispreds(model, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNofXtfSQvh2"
      },
      "source": [
        "### Saving trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9lLO9p1d8mC"
      },
      "outputs": [],
      "source": [
        "def save_model_state_dict(\n",
        "    model: HookedTransformer, \n",
        "    filename: str | None = None\n",
        ") -> None:\n",
        "    if not os.path.isdir(\"models\"):\n",
        "        os.mkdir(\"models\")\n",
        "    if not filename:\n",
        "        timestamp = dt.now().isoformat(\"T\", \"minutes\").replace(\":\", \"-\")\n",
        "        filename = f\"model_state_dict_{timestamp}.pkl\"\n",
        "    with open(f\"models/{filename}\", \"wb\") as f:\n",
        "        pickle.dump(model.state_dict(), f)\n",
        "\n",
        "# save_model_state_dict(model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7o--LiD_HMGs"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
