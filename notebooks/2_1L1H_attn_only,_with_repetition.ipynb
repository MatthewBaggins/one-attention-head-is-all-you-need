{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# One Attention Head Is All You Need for Sorting Fixed-Length Lists [working title]\n",
        "\n",
        "(to justify that title, we may later need to replicate it on eg 100 element lists and bigger `d_vocab`)\n",
        "\n",
        "Old title: Training a Transformer for Sorting Fixed-length Lists\n",
        "\n",
        "This experiment was based on the [suggestion given by Neel Nanda](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj/p/ejtFsvyhRkMofKAFy#Problems)."
      ],
      "metadata": {
        "id": "jKyC3mZ72qlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sorting fixed-length lists\n",
        "  - `START 4 6 2 9 MID 2 4 6 9`\n",
        "  - How does difficulty change with the length of the list?\n",
        "  - A* Sorting variable-length lists.\n",
        "    - What’s the sorting algorithm? What’s the longest list you can get to? How is accuracy affected by longer lists?"
      ],
      "metadata": {
        "id": "tATlby1Z2tvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO\n",
        "\n",
        "- Find the point where model groks validation data\n",
        "- See if it can grok it with smaller n of heads and/or dimensions (both d_head and d_model)\n",
        "- Do interpretability on the smallest viable model and reap profit\n"
      ],
      "metadata": {
        "id": "ws2IHjMAYlxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "DlYDguQD2vGK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqa5uVY-2oC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e4fdbc6-ba09-4938-bc6c-38bc1029e0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (5.5.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly) (8.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/neelnanda-io/TransformerLens\n",
            "  Cloning https://github.com/neelnanda-io/TransformerLens to /tmp/pip-req-build-ay0m7_re\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens /tmp/pip-req-build-ay0m7_re\n",
            "  Resolved https://github.com/neelnanda-io/TransformerLens to commit 090f63afcf72e8ecd9527bbb6f598874554def1b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: rich<13.0.0,>=12.6.0 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (12.6.0)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.7.1 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (2.8.0)\n",
            "Requirement already satisfied: fancy-einsum<0.0.4,>=0.0.3 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (0.0.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.1 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (4.25.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (1.21.6)\n",
            "Requirement already satisfied: wandb<0.14.0,>=0.13.5 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (0.13.9)\n",
            "Requirement already satisfied: torchtyping<0.2.0,>=0.1.4 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (0.1.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.64.1 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (4.64.1)\n",
            "Requirement already satisfied: torch<2.0,>=1.10 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.1.5 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (1.3.5)\n",
            "Requirement already satisfied: einops<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from transformer-lens==0.0.0) (0.6.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (0.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (21.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (0.70.14)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (6.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (3.8.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.1.5->transformer-lens==0.0.0) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0.0,>=12.6.0->transformer-lens==0.0.0) (4.4.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0.0,>=12.6.0->transformer-lens==0.0.0) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0.0,>=12.6.0->transformer-lens==0.0.0) (2.6.1)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.8/dist-packages (from torchtyping<0.2.0,>=0.1.4->transformer-lens==0.0.0) (2.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.25.1->transformer-lens==0.0.0) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.25.1->transformer-lens==0.0.0) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.25.1->transformer-lens==0.0.0) (3.9.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (1.13.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (57.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (1.3.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (3.1.30)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (0.4.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (1.8.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (4.0.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->transformer-lens==0.0.0) (2.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.14.0,>=0.13.5->transformer-lens==0.0.0) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy torch tqdm pandas plotly typing_extensions\n",
        "!pip install git+https://github.com/neelnanda-io/TransformerLens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime as dt\n",
        "from itertools import repeat\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from typing import cast, Generator, Literal\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, tensor, Tensor, TensorType as TT\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
        "from typing_extensions import Self"
      ],
      "metadata": {
        "id": "dqFfW5V32yaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install circuitsvis\n",
        "import circuitsvis as cv\n",
        "# Testing that the library works\n",
        "cv.examples.hello(\"Neel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "vFkDveZyptS5",
        "outputId": "d255d846-37d2-4192-897b-f3d1423205d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.8/dist-packages (1.38.1)\n",
            "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /usr/local/lib/python3.8/dist-packages (from circuitsvis) (5.2.0)\n",
            "Requirement already satisfied: torch<2.0,>=1.10 in /usr/local/lib/python3.8/dist-packages (from circuitsvis) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.8/dist-packages (from circuitsvis) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<2.0,>=1.10->circuitsvis) (4.4.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f2a7bcdc550>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-760afd50-7a29\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.38.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-760afd50-7a29\",\n",
              "      Hello,\n",
              "      {\"name\": \"Neel\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparams"
      ],
      "metadata": {
        "id": "Oo4oP4dSrYhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"{DEVICE = }\")\n",
        "\n",
        "# Seeds to generate training, validation, and test data\n",
        "TRAIN_SEED = 42\n",
        "VAL_SEED = 66\n",
        "TEST_SEED = 1729\n",
        "\n",
        "# Fixed length of list to be sorted\n",
        "LIST_LENGTH = 5\n",
        "# Context length: [start, *(unsorted_)list_length, mid, *(sorted_)list_length]\n",
        "N_CTX = 2 * LIST_LENGTH + 2\n",
        "\n",
        "# Size of vocabulary\n",
        "D_VOCAB = 66\n",
        "\n",
        "# \"Real\" tokens range from 0 to D_VOCAB - 2 (non-inclusive)\n",
        "VOCAB_MIN_ID = 0\n",
        "VOCAB_MAX_ID = D_VOCAB - 2\n",
        "\n",
        "# START token is D_VOCAB - 2 and MID token is D_VOCAB - 1\n",
        "START_TOKEN_ID = VOCAB_MAX_ID\n",
        "MID_TOKEN_ID = D_VOCAB - 1\n",
        "\n",
        "# Should lists have repetitions?\n",
        "ALLOW_REPETITIONS = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XwivtyM_Hru",
        "outputId": "919aa12f-e01d-410f-c7f3-00c11c6b5b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE = 'cpu'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toy model for testing"
      ],
      "metadata": {
        "id": "N2CyZy6c_CDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_toy_model() -> HookedTransformer:\n",
        "    \"\"\"Make a toy transformer for testing functions\"\"\"\n",
        "    cfg = HookedTransformerConfig(\n",
        "        n_layers=1,\n",
        "        d_model=32,\n",
        "        n_ctx=N_CTX,\n",
        "        d_head=8,\n",
        "        n_heads=1,\n",
        "        d_vocab=D_VOCAB,\n",
        "        act_fn=\"relu\",\n",
        "        device=DEVICE,\n",
        "        attn_only=True\n",
        "    )\n",
        "    return HookedTransformer(cfg, move_to_device=True)"
      ],
      "metadata": {
        "id": "ixpOjGbF_Dda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data generator and datasets"
      ],
      "metadata": {
        "id": "dlWGwox63j24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_list(batch_size: int) -> Tensor:\n",
        "    if ALLOW_REPETITIONS:\n",
        "        return torch.randint(VOCAB_MIN_ID, VOCAB_MAX_ID, (batch_size, LIST_LENGTH)).to(DEVICE)\n",
        "    return tensor([\n",
        "        random.sample(range(VOCAB_MIN_ID, VOCAB_MAX_ID), k=LIST_LENGTH) \n",
        "        for _ in range(batch_size)\n",
        "    ]).to(DEVICE)\n",
        "\n",
        "# General generator\n",
        "def make_data_gen(\n",
        "    *,\n",
        "    batch_size: int = 32,\n",
        "    dataset: Literal[\"train\", \"val\", \"test\"], # probably this arg needs a better name,\n",
        ") -> Generator[Tensor, None, None]:\n",
        "    assert dataset in (\"train\", \"val\", \"test\")\n",
        "    if dataset == \"train\":\n",
        "        seed = TRAIN_SEED\n",
        "    elif dataset == \"val\":\n",
        "        seed = VAL_SEED\n",
        "    else: # test\n",
        "        seed = TEST_SEED\n",
        "    torch.manual_seed(seed)\n",
        "    while True:\n",
        "        # Generate random numbers\n",
        "        x = generate_list(batch_size)\n",
        "        # Sort\n",
        "        x_sorted = torch.sort(x, dim=1).values\n",
        "        # START tokens\n",
        "        x_start = START_TOKEN_ID * torch.ones(batch_size, dtype=torch.int32).reshape(batch_size, -1).to(DEVICE)\n",
        "        # MID tokens\n",
        "        x_mid = MID_TOKEN_ID * torch.ones(batch_size, dtype=torch.int32).reshape(batch_size, -1).to(DEVICE)\n",
        "        yield torch.cat((x_start, x, x_mid, x_sorted), dim=1)\n",
        "\n",
        "\n",
        "# Training data generator (kinda wrapper)\n",
        "def make_train_gen() -> Generator[Tensor, None, None]:\n",
        "    \"\"\"Make generator of training data\"\"\"\n",
        "    return make_data_gen(batch_size=128, dataset=\"train\")\n",
        "\n",
        "# Validation and test data\n",
        "\n",
        "val_data = next(make_data_gen(batch_size=1000, dataset=\"val\"))\n",
        "test_data = next(make_data_gen(batch_size=1000, dataset=\"test\"))\n",
        "print(f\"{val_data.shape=}; {test_data.shape=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44OZjOq83k1H",
        "outputId": "e16fe74f-ea18-4daa-b95b-5ef9df729e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_data.shape=torch.Size([1000, 12]); test_data.shape=torch.Size([1000, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def has_repetition(l):\n",
        "  return len(l.tolist()) != len(set(l.tolist()))\n",
        "\n",
        "def make_data_gen_with_rep(\n",
        "    *,\n",
        "    batch_size: int = 32,\n",
        "    dataset: Literal[\"train\", \"val\", \"test\"], # probably this arg needs a better name,\n",
        ") -> Generator[Tensor, None, None]:\n",
        "    assert dataset in (\"train\", \"val\", \"test\")\n",
        "    if dataset == \"train\":\n",
        "        seed = TRAIN_SEED\n",
        "    elif dataset == \"val\":\n",
        "        seed = VAL_SEED\n",
        "    else: # test\n",
        "        seed = TEST_SEED\n",
        "    torch.manual_seed(seed)\n",
        "    data_gen = make_data_gen(batch_size = 1, dataset=dataset)\n",
        "    while True:\n",
        "      u = []\n",
        "      for _ in range(batch_size):\n",
        "        next_try = next(data_gen)\n",
        "        while not has_repetition(next_try[0][1:LIST_LENGTH+1]):\n",
        "          next_try = next(data_gen)\n",
        "        u.append(next_try)\n",
        "\n",
        "      yield torch.cat(u)"
      ],
      "metadata": {
        "id": "U853p9b9yLmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = torch.tensor([1,2,3,3]).tolist()\n",
        "torch.tensor([l,l,l])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_qsk7a63Lzb",
        "outputId": "c7225d41-6126-4e9c-b7b8-a68eff1dfe96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3, 3],\n",
              "        [1, 2, 3, 3],\n",
              "        [1, 2, 3, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = make_data_gen_with_rep(batch_size=10, dataset=\"test\")\n",
        "next(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhFcwQ0W2CQv",
        "outputId": "62aceb2c-6d4a-4b60-881b-21179f62fb08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[64, 30,  1,  3,  1, 37, 65,  1,  1,  3, 30, 37],\n",
              "        [64, 39, 51,  9, 26, 51, 65,  9, 26, 39, 51, 51],\n",
              "        [64, 47, 16,  0, 45, 47, 65,  0, 16, 45, 47, 47],\n",
              "        [64,  4, 25, 25, 38, 53, 65,  4, 25, 25, 38, 53],\n",
              "        [64, 42, 57, 47, 55, 42, 65, 42, 42, 47, 55, 57],\n",
              "        [64, 46, 47, 28, 28, 41, 65, 28, 28, 41, 46, 47],\n",
              "        [64, 51, 31,  1, 31, 54, 65,  1, 31, 31, 51, 54],\n",
              "        [64, 50, 57, 57,  8, 14, 65,  8, 14, 50, 57, 57],\n",
              "        [64,  3, 62,  2, 57, 57, 65,  2,  3, 57, 57, 62],\n",
              "        [64,  4,  4,  6, 62, 17, 65,  4,  4,  6, 17, 62]])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "toy_model = make_toy_model()\n",
        "x = next(make_train_gen())\n",
        "y = toy_model(x)\n",
        "y.argmax(-1)[0], x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pty0d4ht0re",
        "outputId": "78eea155-bcda-4fcd-c057-b19237f8e23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0, 10, 43, 17, 55, 27, 54, 43, 29,  0, 53, 54]),\n",
              " tensor([64, 38, 51, 28, 14, 42, 65, 14, 28, 38, 42, 51]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "id": "hBJn1fKm3pMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(\n",
        "    logits: Tensor, # [batch, pos, d_vocab] \n",
        "    tokens: Tensor, # [batch, pos] \n",
        "    return_per_token: bool = False\n",
        ") -> Tensor:\n",
        "    #TODO: document what happens here\n",
        "    sorted_start_pos = LIST_LENGTH + 2\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    # print(tokens[0])\n",
        "    tokens = tokens[:, sorted_start_pos : None]\n",
        "    # print(tokens[0], logits.shape,tokens.shape, logits.argmax(-1)[0])\n",
        "    log_probs = logits.log_softmax(-1)\n",
        "    correct_log_probs = log_probs.gather(-1, tokens[..., None])[..., 0]\n",
        "    if return_per_token:\n",
        "        return -correct_log_probs\n",
        "    return -correct_log_probs.mean()\n",
        "\n",
        "# Test\n",
        "\n",
        "toy_model = make_toy_model()\n",
        "x = next(make_train_gen())\n",
        "gen = make_train_gen()\n",
        "x = next(gen)\n",
        "y = toy_model(x)\n",
        "loss_fn(y, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTlNfaKY3qkd",
        "outputId": "b4edcbd6-03c7-4e60-9d8c-6e57a1bffa1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.4752, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy and validation"
      ],
      "metadata": {
        "id": "GKKrzLEq3sun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_diff_row_inds(\n",
        "    a: Tensor, # [dim1, dim2]\n",
        "    b: Tensor  # [dim1, dim2]\n",
        ") -> Tensor:   # [dim1]\n",
        "    \"\"\"Find indices of rows where a and b differ\"\"\"\n",
        "    assert a.shape == b.shape\n",
        "    return ((a == b).prod(dim=1) == 0).nonzero(as_tuple=True)[0]\n",
        "\n",
        "def acc_fn(\n",
        "    logits: Tensor, # [batch, pos, d_vocab]\n",
        "    tokens: Tensor, # [batch, pos]\n",
        "    per: Literal[\"token\", \"sequence\"] = \"sequence\"\n",
        ") -> float:\n",
        "    \"\"\"Compute accuracy as percentage of correct predictions\"\"\"\n",
        "    sorted_start_pos = LIST_LENGTH + 2\n",
        "    # Get logits of predictions for position\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    preds = logits.argmax(-1)\n",
        "    tokens = tokens[:, sorted_start_pos:]\n",
        "    if per == \"sequence\":\n",
        "        return (preds == tokens).prod(dim=1).float().mean().item()\n",
        "    return (preds == tokens).float().mean().item()\n",
        "\n",
        "def validate(\n",
        "    model: HookedTransformer, \n",
        "    data: Tensor # [batch, pos]\n",
        ") -> float:\n",
        "    \"\"\"Test this model on `data`\"\"\"\n",
        "    logits = model(data)\n",
        "    acc = acc_fn(logits, tokens=data)\n",
        "    return acc\n",
        "\n",
        "def show_mispreds(\n",
        "    model: HookedTransformer, \n",
        "    data: Tensor # [batch, pos]\n",
        ") -> None:\n",
        "    \"\"\"Test this model on `data` and print mispredictions\"\"\"\n",
        "    logits = model(data)\n",
        "    sorted_start_pos = LIST_LENGTH + 2\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    tokens = data[:, sorted_start_pos:]\n",
        "    preds = logits.argmax(-1)\n",
        "    mispred_inds = get_diff_row_inds(tokens, preds)\n",
        "    for i in mispred_inds:\n",
        "        print(f\"[{i}] {tokens[i].numpy().tolist()} | {preds[i].numpy().tolist()}\")\n",
        "    print(f\"{len(mispred_inds)}/{len(preds)} ({len(mispred_inds) / len(preds) :.2%})\")\n",
        "    \n",
        "# show_mispreds(toy_model, val_data[:10])    \n",
        "\n",
        "\n",
        "# Test\n",
        "#TODO:\n"
      ],
      "metadata": {
        "id": "1x6q7b7O3rZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and config"
      ],
      "metadata": {
        "id": "9emjjkpT3gHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = HookedTransformerConfig(\n",
        "    d_model=128,\n",
        "    n_layers=1,\n",
        "    n_heads=1,\n",
        "    d_head=32,\n",
        "    n_ctx=N_CTX,\n",
        "    d_vocab=D_VOCAB,\n",
        "    act_fn=\"relu\",\n",
        "    seed=42,\n",
        "    device=DEVICE,\n",
        "    attn_only=True\n",
        ")\n",
        "model = HookedTransformer(cfg, move_to_device=True)"
      ],
      "metadata": {
        "id": "nX3rzs1j3V0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training setup"
      ],
      "metadata": {
        "id": "VLbAR2rx8swT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of epochs\n",
        "n_epochs = 20000\n",
        "\n",
        "# Optimization\n",
        "lr = 1e-3\n",
        "betas = (.9, .999)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \"min\", patience=10000)\n",
        "\n",
        "# Training data generator\n",
        "train_gen = make_train_gen()"
      ],
      "metadata": {
        "id": "6qkMZ_Se8uLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "9vYV7QAd3vCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "def converged(val_accs: list[float], n_last: int = 10) -> bool:\n",
        "    return cast(bool, (tensor(val_accs[-n_last:]) == 1).all().item())\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    tokens = next(train_gen).to(device=DEVICE)\n",
        "    logits = model(tokens)\n",
        "    loss = loss_fn(logits, tokens)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    scheduler.step(loss)\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        losses.append(loss.item())\n",
        "        train_batch_acc = acc_fn(logits, tokens)\n",
        "        val_acc = validate(model, val_data)\n",
        "        val_loss = loss_fn(model(val_data), val_data)\n",
        "\n",
        "        train_accuracies.append(train_batch_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{n_epochs} ({epoch / n_epochs:.0%}) : \"\n",
        "            f\"loss = {loss.item():.4f}; {train_batch_acc=:.3%}; \"\n",
        "            f\"{val_acc=:.3%}; lr={scheduler._last_lr[0]}\" #type:ignore\n",
        "        )\n",
        "        # If last 10 recorded val_accuracies are 100%\n",
        "        if converged(val_accuracies):\n",
        "            print(f\"\\nAchieved consistent perfect validation accuracy after {epoch} epochs\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHBPL3Xw3uAt",
        "outputId": "46946138-c68e-4090-d729-36051ce38391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/20000 (0%) : loss = 4.4764; train_batch_acc=0.000%; val_acc=0.000%; lr=0.001\n",
            "Epoch 100/20000 (0%) : loss = 0.4194; train_batch_acc=53.906%; val_acc=53.000%; lr=0.001\n",
            "Epoch 200/20000 (1%) : loss = 0.1928; train_batch_acc=72.656%; val_acc=78.800%; lr=0.001\n",
            "Epoch 300/20000 (2%) : loss = 0.1562; train_batch_acc=83.594%; val_acc=85.600%; lr=0.001\n",
            "Epoch 400/20000 (2%) : loss = 0.1257; train_batch_acc=85.938%; val_acc=86.200%; lr=0.001\n",
            "Epoch 500/20000 (2%) : loss = 0.1202; train_batch_acc=87.500%; val_acc=88.100%; lr=0.001\n",
            "Epoch 600/20000 (3%) : loss = 0.0971; train_batch_acc=88.281%; val_acc=90.200%; lr=0.001\n",
            "Epoch 700/20000 (4%) : loss = 0.0720; train_batch_acc=92.188%; val_acc=88.800%; lr=0.001\n",
            "Epoch 800/20000 (4%) : loss = 0.0695; train_batch_acc=92.969%; val_acc=90.500%; lr=0.001\n",
            "Epoch 900/20000 (4%) : loss = 0.0749; train_batch_acc=90.625%; val_acc=90.500%; lr=0.001\n",
            "Epoch 1000/20000 (5%) : loss = 0.0514; train_batch_acc=92.188%; val_acc=90.200%; lr=0.001\n",
            "Epoch 1100/20000 (6%) : loss = 0.0671; train_batch_acc=89.844%; val_acc=91.500%; lr=0.001\n",
            "Epoch 1200/20000 (6%) : loss = 0.0529; train_batch_acc=93.750%; val_acc=91.100%; lr=0.001\n",
            "Epoch 1300/20000 (6%) : loss = 0.0768; train_batch_acc=87.500%; val_acc=90.500%; lr=0.001\n",
            "Epoch 1400/20000 (7%) : loss = 0.0735; train_batch_acc=91.406%; val_acc=91.500%; lr=0.001\n",
            "Epoch 1500/20000 (8%) : loss = 0.0672; train_batch_acc=90.625%; val_acc=90.300%; lr=0.001\n",
            "Epoch 1600/20000 (8%) : loss = 0.0337; train_batch_acc=93.750%; val_acc=92.000%; lr=0.001\n",
            "Epoch 1700/20000 (8%) : loss = 0.0810; train_batch_acc=89.062%; val_acc=91.800%; lr=0.001\n",
            "Epoch 1800/20000 (9%) : loss = 0.0701; train_batch_acc=88.281%; val_acc=91.300%; lr=0.001\n",
            "Epoch 1900/20000 (10%) : loss = 0.0545; train_batch_acc=90.625%; val_acc=93.100%; lr=0.001\n",
            "Epoch 2000/20000 (10%) : loss = 0.0351; train_batch_acc=93.750%; val_acc=93.300%; lr=0.001\n",
            "Epoch 2100/20000 (10%) : loss = 0.0502; train_batch_acc=94.531%; val_acc=92.300%; lr=0.001\n",
            "Epoch 2200/20000 (11%) : loss = 0.0458; train_batch_acc=92.969%; val_acc=91.800%; lr=0.001\n",
            "Epoch 2300/20000 (12%) : loss = 0.0466; train_batch_acc=92.969%; val_acc=93.600%; lr=0.001\n",
            "Epoch 2400/20000 (12%) : loss = 0.0742; train_batch_acc=91.406%; val_acc=93.300%; lr=0.001\n",
            "Epoch 2500/20000 (12%) : loss = 0.0514; train_batch_acc=91.406%; val_acc=93.500%; lr=0.001\n",
            "Epoch 2600/20000 (13%) : loss = 0.0560; train_batch_acc=90.625%; val_acc=93.300%; lr=0.001\n",
            "Epoch 2700/20000 (14%) : loss = 0.0618; train_batch_acc=94.531%; val_acc=93.400%; lr=0.001\n",
            "Epoch 2800/20000 (14%) : loss = 0.0435; train_batch_acc=92.969%; val_acc=93.600%; lr=0.001\n",
            "Epoch 2900/20000 (14%) : loss = 0.0262; train_batch_acc=93.750%; val_acc=93.200%; lr=0.001\n",
            "Epoch 3000/20000 (15%) : loss = 0.0605; train_batch_acc=93.750%; val_acc=93.700%; lr=0.001\n",
            "Epoch 3100/20000 (16%) : loss = 0.0280; train_batch_acc=97.656%; val_acc=93.700%; lr=0.001\n",
            "Epoch 3200/20000 (16%) : loss = 0.0216; train_batch_acc=97.656%; val_acc=94.200%; lr=0.001\n",
            "Epoch 3300/20000 (16%) : loss = 0.0324; train_batch_acc=96.094%; val_acc=95.200%; lr=0.001\n",
            "Epoch 3400/20000 (17%) : loss = 0.0441; train_batch_acc=93.750%; val_acc=94.100%; lr=0.001\n",
            "Epoch 3500/20000 (18%) : loss = 0.0360; train_batch_acc=96.875%; val_acc=94.900%; lr=0.001\n",
            "Epoch 3600/20000 (18%) : loss = 0.0252; train_batch_acc=96.094%; val_acc=95.400%; lr=0.001\n",
            "Epoch 3700/20000 (18%) : loss = 0.0791; train_batch_acc=92.188%; val_acc=94.000%; lr=0.001\n",
            "Epoch 3800/20000 (19%) : loss = 0.0452; train_batch_acc=94.531%; val_acc=95.300%; lr=0.001\n",
            "Epoch 3900/20000 (20%) : loss = 0.0321; train_batch_acc=95.312%; val_acc=96.000%; lr=0.001\n",
            "Epoch 4000/20000 (20%) : loss = 0.0312; train_batch_acc=95.312%; val_acc=95.400%; lr=0.001\n",
            "Epoch 4100/20000 (20%) : loss = 0.0190; train_batch_acc=97.656%; val_acc=95.100%; lr=0.001\n",
            "Epoch 4200/20000 (21%) : loss = 0.0285; train_batch_acc=96.875%; val_acc=94.900%; lr=0.001\n",
            "Epoch 4300/20000 (22%) : loss = 0.0317; train_batch_acc=95.312%; val_acc=94.700%; lr=0.001\n",
            "Epoch 4400/20000 (22%) : loss = 0.0280; train_batch_acc=94.531%; val_acc=95.700%; lr=0.001\n",
            "Epoch 4500/20000 (22%) : loss = 0.0218; train_batch_acc=96.875%; val_acc=95.400%; lr=0.001\n",
            "Epoch 4600/20000 (23%) : loss = 0.0297; train_batch_acc=96.875%; val_acc=95.300%; lr=0.001\n",
            "Epoch 4700/20000 (24%) : loss = 0.0216; train_batch_acc=96.875%; val_acc=96.100%; lr=0.001\n",
            "Epoch 4800/20000 (24%) : loss = 0.0284; train_batch_acc=96.875%; val_acc=95.300%; lr=0.001\n",
            "Epoch 4900/20000 (24%) : loss = 0.0185; train_batch_acc=96.875%; val_acc=95.300%; lr=0.001\n",
            "Epoch 5000/20000 (25%) : loss = 0.0258; train_batch_acc=94.531%; val_acc=95.700%; lr=0.001\n",
            "Epoch 5100/20000 (26%) : loss = 0.0251; train_batch_acc=96.875%; val_acc=95.600%; lr=0.001\n",
            "Epoch 5200/20000 (26%) : loss = 0.0530; train_batch_acc=92.188%; val_acc=96.500%; lr=0.001\n",
            "Epoch 5300/20000 (26%) : loss = 0.0200; train_batch_acc=97.656%; val_acc=97.000%; lr=0.001\n",
            "Epoch 5400/20000 (27%) : loss = 0.0286; train_batch_acc=97.656%; val_acc=95.400%; lr=0.001\n",
            "Epoch 5500/20000 (28%) : loss = 0.0254; train_batch_acc=97.656%; val_acc=96.200%; lr=0.001\n",
            "Epoch 5600/20000 (28%) : loss = 0.0221; train_batch_acc=96.094%; val_acc=96.100%; lr=0.001\n",
            "Epoch 5700/20000 (28%) : loss = 0.0185; train_batch_acc=95.312%; val_acc=96.800%; lr=0.001\n",
            "Epoch 5800/20000 (29%) : loss = 0.0205; train_batch_acc=96.875%; val_acc=95.500%; lr=0.001\n",
            "Epoch 5900/20000 (30%) : loss = 0.0120; train_batch_acc=98.438%; val_acc=96.700%; lr=0.001\n",
            "Epoch 6000/20000 (30%) : loss = 0.0362; train_batch_acc=96.094%; val_acc=95.700%; lr=0.001\n",
            "Epoch 6100/20000 (30%) : loss = 0.0330; train_batch_acc=95.312%; val_acc=96.500%; lr=0.001\n",
            "Epoch 6200/20000 (31%) : loss = 0.0183; train_batch_acc=97.656%; val_acc=96.800%; lr=0.001\n",
            "Epoch 6300/20000 (32%) : loss = 0.0153; train_batch_acc=98.438%; val_acc=96.800%; lr=0.001\n",
            "Epoch 6400/20000 (32%) : loss = 0.0231; train_batch_acc=94.531%; val_acc=95.700%; lr=0.001\n",
            "Epoch 6500/20000 (32%) : loss = 0.0557; train_batch_acc=92.188%; val_acc=96.300%; lr=0.001\n",
            "Epoch 6600/20000 (33%) : loss = 0.0191; train_batch_acc=97.656%; val_acc=95.800%; lr=0.001\n",
            "Epoch 6700/20000 (34%) : loss = 0.0109; train_batch_acc=99.219%; val_acc=97.900%; lr=0.0001\n",
            "Epoch 6800/20000 (34%) : loss = 0.0119; train_batch_acc=98.438%; val_acc=98.400%; lr=0.0001\n",
            "Epoch 6900/20000 (34%) : loss = 0.0143; train_batch_acc=98.438%; val_acc=98.200%; lr=0.0001\n",
            "Epoch 7000/20000 (35%) : loss = 0.0107; train_batch_acc=98.438%; val_acc=98.100%; lr=0.0001\n",
            "Epoch 7100/20000 (36%) : loss = 0.0098; train_batch_acc=99.219%; val_acc=98.800%; lr=0.0001\n",
            "Epoch 7200/20000 (36%) : loss = 0.0088; train_batch_acc=99.219%; val_acc=98.400%; lr=0.0001\n",
            "Epoch 7300/20000 (36%) : loss = 0.0062; train_batch_acc=99.219%; val_acc=98.200%; lr=0.0001\n",
            "Epoch 7400/20000 (37%) : loss = 0.0085; train_batch_acc=99.219%; val_acc=98.700%; lr=0.0001\n",
            "Epoch 7500/20000 (38%) : loss = 0.0047; train_batch_acc=100.000%; val_acc=98.300%; lr=0.0001\n",
            "Epoch 7600/20000 (38%) : loss = 0.0121; train_batch_acc=99.219%; val_acc=98.500%; lr=0.0001\n",
            "Epoch 7700/20000 (38%) : loss = 0.0067; train_batch_acc=99.219%; val_acc=98.400%; lr=0.0001\n",
            "Epoch 7800/20000 (39%) : loss = 0.0256; train_batch_acc=96.875%; val_acc=98.600%; lr=0.0001\n",
            "Epoch 7900/20000 (40%) : loss = 0.0083; train_batch_acc=99.219%; val_acc=98.500%; lr=0.0001\n",
            "Epoch 8000/20000 (40%) : loss = 0.0110; train_batch_acc=97.656%; val_acc=98.300%; lr=0.0001\n",
            "Epoch 8100/20000 (40%) : loss = 0.0156; train_batch_acc=97.656%; val_acc=98.500%; lr=0.0001\n",
            "Epoch 8200/20000 (41%) : loss = 0.0070; train_batch_acc=99.219%; val_acc=98.600%; lr=0.0001\n",
            "Epoch 8300/20000 (42%) : loss = 0.0048; train_batch_acc=99.219%; val_acc=98.300%; lr=0.0001\n",
            "Epoch 8400/20000 (42%) : loss = 0.0194; train_batch_acc=96.875%; val_acc=98.400%; lr=0.0001\n",
            "Epoch 8500/20000 (42%) : loss = 0.0080; train_batch_acc=98.438%; val_acc=98.700%; lr=0.0001\n",
            "Epoch 8600/20000 (43%) : loss = 0.0042; train_batch_acc=99.219%; val_acc=98.600%; lr=0.0001\n",
            "Epoch 8700/20000 (44%) : loss = 0.0066; train_batch_acc=99.219%; val_acc=98.600%; lr=0.0001\n",
            "Epoch 8800/20000 (44%) : loss = 0.0084; train_batch_acc=99.219%; val_acc=98.700%; lr=0.0001\n",
            "Epoch 8900/20000 (44%) : loss = 0.0028; train_batch_acc=100.000%; val_acc=98.700%; lr=0.0001\n",
            "Epoch 9000/20000 (45%) : loss = 0.0106; train_batch_acc=99.219%; val_acc=98.500%; lr=0.0001\n",
            "Epoch 9100/20000 (46%) : loss = 0.0059; train_batch_acc=99.219%; val_acc=98.500%; lr=0.0001\n",
            "Epoch 9200/20000 (46%) : loss = 0.0028; train_batch_acc=100.000%; val_acc=98.700%; lr=0.0001\n",
            "Epoch 9300/20000 (46%) : loss = 0.0048; train_batch_acc=99.219%; val_acc=98.400%; lr=0.0001\n",
            "Epoch 9400/20000 (47%) : loss = 0.0098; train_batch_acc=99.219%; val_acc=98.800%; lr=0.0001\n",
            "Epoch 9500/20000 (48%) : loss = 0.0083; train_batch_acc=97.656%; val_acc=98.500%; lr=0.0001\n",
            "Epoch 9600/20000 (48%) : loss = 0.0092; train_batch_acc=99.219%; val_acc=98.900%; lr=1e-05\n",
            "Epoch 9700/20000 (48%) : loss = 0.0111; train_batch_acc=98.438%; val_acc=99.000%; lr=1e-05\n",
            "Epoch 9800/20000 (49%) : loss = 0.0115; train_batch_acc=98.438%; val_acc=98.800%; lr=1e-05\n",
            "Epoch 9900/20000 (50%) : loss = 0.0175; train_batch_acc=97.656%; val_acc=98.900%; lr=1e-05\n",
            "Epoch 10000/20000 (50%) : loss = 0.0122; train_batch_acc=98.438%; val_acc=99.000%; lr=1e-05\n",
            "Epoch 10100/20000 (50%) : loss = 0.0096; train_batch_acc=99.219%; val_acc=99.000%; lr=1e-05\n",
            "Epoch 10200/20000 (51%) : loss = 0.0044; train_batch_acc=99.219%; val_acc=99.000%; lr=1e-05\n",
            "Epoch 10300/20000 (52%) : loss = 0.0179; train_batch_acc=97.656%; val_acc=98.800%; lr=1e-05\n",
            "Epoch 10400/20000 (52%) : loss = 0.0099; train_batch_acc=98.438%; val_acc=98.900%; lr=1e-05\n",
            "Epoch 10500/20000 (52%) : loss = 0.0118; train_batch_acc=98.438%; val_acc=98.900%; lr=1e-05\n",
            "Epoch 10600/20000 (53%) : loss = 0.0050; train_batch_acc=99.219%; val_acc=98.800%; lr=1e-05\n",
            "Epoch 10700/20000 (54%) : loss = 0.0043; train_batch_acc=100.000%; val_acc=98.700%; lr=1e-05\n",
            "Epoch 10800/20000 (54%) : loss = 0.0121; train_batch_acc=96.875%; val_acc=98.900%; lr=1e-05\n",
            "Epoch 10900/20000 (55%) : loss = 0.0062; train_batch_acc=98.438%; val_acc=98.900%; lr=1e-05\n",
            "Epoch 11000/20000 (55%) : loss = 0.0192; train_batch_acc=96.875%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 11100/20000 (56%) : loss = 0.0046; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 11200/20000 (56%) : loss = 0.0042; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 11300/20000 (56%) : loss = 0.0076; train_batch_acc=97.656%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 11400/20000 (57%) : loss = 0.0100; train_batch_acc=96.875%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 11500/20000 (57%) : loss = 0.0039; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 11600/20000 (58%) : loss = 0.0040; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 11700/20000 (58%) : loss = 0.0048; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 11800/20000 (59%) : loss = 0.0135; train_batch_acc=97.656%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 11900/20000 (60%) : loss = 0.0080; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 12000/20000 (60%) : loss = 0.0110; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 12100/20000 (60%) : loss = 0.0153; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 12200/20000 (61%) : loss = 0.0044; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 12300/20000 (62%) : loss = 0.0032; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 12400/20000 (62%) : loss = 0.0102; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 12500/20000 (62%) : loss = 0.0082; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 12600/20000 (63%) : loss = 0.0157; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 12700/20000 (64%) : loss = 0.0047; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000002e-06\n",
            "Epoch 12800/20000 (64%) : loss = 0.0071; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 12900/20000 (64%) : loss = 0.0101; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13000/20000 (65%) : loss = 0.0149; train_batch_acc=96.875%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13100/20000 (66%) : loss = 0.0075; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13200/20000 (66%) : loss = 0.0048; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13300/20000 (66%) : loss = 0.0090; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13400/20000 (67%) : loss = 0.0077; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13500/20000 (68%) : loss = 0.0119; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13600/20000 (68%) : loss = 0.0042; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13700/20000 (68%) : loss = 0.0053; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13800/20000 (69%) : loss = 0.0111; train_batch_acc=97.656%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 13900/20000 (70%) : loss = 0.0035; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 14000/20000 (70%) : loss = 0.0066; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 14100/20000 (70%) : loss = 0.0116; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 14200/20000 (71%) : loss = 0.0085; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 14300/20000 (72%) : loss = 0.0073; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 14400/20000 (72%) : loss = 0.0041; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 14500/20000 (72%) : loss = 0.0070; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 14600/20000 (73%) : loss = 0.0042; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000002e-07\n",
            "Epoch 14700/20000 (74%) : loss = 0.0074; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 14800/20000 (74%) : loss = 0.0079; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 14900/20000 (74%) : loss = 0.0045; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15000/20000 (75%) : loss = 0.0074; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15100/20000 (76%) : loss = 0.0053; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15200/20000 (76%) : loss = 0.0096; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15300/20000 (76%) : loss = 0.0055; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15400/20000 (77%) : loss = 0.0147; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15500/20000 (78%) : loss = 0.0116; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15600/20000 (78%) : loss = 0.0134; train_batch_acc=97.656%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15700/20000 (78%) : loss = 0.0034; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15800/20000 (79%) : loss = 0.0049; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 15900/20000 (80%) : loss = 0.0116; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16000/20000 (80%) : loss = 0.0053; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16100/20000 (80%) : loss = 0.0072; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16200/20000 (81%) : loss = 0.0081; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16300/20000 (82%) : loss = 0.0110; train_batch_acc=97.656%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16400/20000 (82%) : loss = 0.0097; train_batch_acc=97.656%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16500/20000 (82%) : loss = 0.0097; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16600/20000 (83%) : loss = 0.0080; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16700/20000 (84%) : loss = 0.0045; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16800/20000 (84%) : loss = 0.0051; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 16900/20000 (84%) : loss = 0.0150; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17000/20000 (85%) : loss = 0.0083; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17100/20000 (86%) : loss = 0.0079; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17200/20000 (86%) : loss = 0.0050; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17300/20000 (86%) : loss = 0.0114; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17400/20000 (87%) : loss = 0.0090; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17500/20000 (88%) : loss = 0.0072; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17600/20000 (88%) : loss = 0.0099; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17700/20000 (88%) : loss = 0.0170; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17800/20000 (89%) : loss = 0.0106; train_batch_acc=97.656%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 17900/20000 (90%) : loss = 0.0081; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18000/20000 (90%) : loss = 0.0071; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18100/20000 (90%) : loss = 0.0058; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18200/20000 (91%) : loss = 0.0064; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18300/20000 (92%) : loss = 0.0053; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18400/20000 (92%) : loss = 0.0219; train_batch_acc=96.094%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18500/20000 (92%) : loss = 0.0097; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18600/20000 (93%) : loss = 0.0057; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18700/20000 (94%) : loss = 0.0057; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18800/20000 (94%) : loss = 0.0092; train_batch_acc=97.656%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 18900/20000 (94%) : loss = 0.0170; train_batch_acc=97.656%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19000/20000 (95%) : loss = 0.0041; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19100/20000 (96%) : loss = 0.0040; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19200/20000 (96%) : loss = 0.0060; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19300/20000 (96%) : loss = 0.0059; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19400/20000 (97%) : loss = 0.0103; train_batch_acc=98.438%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19500/20000 (98%) : loss = 0.0044; train_batch_acc=100.000%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19600/20000 (98%) : loss = 0.0084; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19700/20000 (98%) : loss = 0.0111; train_batch_acc=96.875%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19800/20000 (99%) : loss = 0.0064; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n",
            "Epoch 19900/20000 (100%) : loss = 0.0063; train_batch_acc=99.219%; val_acc=98.900%; lr=1.0000000000000004e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing post-training"
      ],
      "metadata": {
        "id": "HABWttcH34dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Validating on validation data:\")\n",
        "val_acc = validate(model, val_data)\n",
        "print(f\"\\t{val_acc=:.3%}\\n\")\n",
        "if val_acc < 1:\n",
        "    show_mispreds(model, val_data)\n",
        "\n",
        "print(\"\\nValidating on test data:\")\n",
        "test_acc = validate(model, test_data)\n",
        "print(f\"\\t{test_acc=:.3%}\\n\")\n",
        "if test_acc < 1:\n",
        "    show_mispreds(model, test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lakshPyuwdO7",
        "outputId": "6128af7f-3893-4ba7-d991-f2745fadf13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating on validation data:\n",
            "\tval_acc=98.900%\n",
            "\n",
            "[2] [10, 13, 13, 16, 39] | [10, 13, 16, 16, 39]\n",
            "[12] [7, 30, 30, 42, 53] | [7, 30, 42, 42, 53]\n",
            "[104] [8, 11, 61, 62, 63] | [8, 11, 60, 62, 63]\n",
            "[171] [1, 21, 23, 23, 25] | [1, 21, 23, 25, 25]\n",
            "[210] [20, 24, 24, 41, 42] | [20, 24, 41, 41, 42]\n",
            "[260] [17, 17, 17, 18, 23] | [17, 17, 17, 17, 23]\n",
            "[592] [22, 22, 29, 56, 62] | [22, 22, 22, 56, 62]\n",
            "[657] [10, 10, 12, 19, 22] | [10, 12, 12, 19, 22]\n",
            "[845] [2, 14, 14, 18, 19] | [2, 14, 18, 18, 19]\n",
            "[912] [17, 17, 19, 32, 57] | [17, 17, 17, 32, 57]\n",
            "[978] [2, 3, 3, 29, 57] | [2, 3, 2, 29, 57]\n",
            "11/1000 (1.10%)\n",
            "\n",
            "Validating on test data:\n",
            "\ttest_acc=99.000%\n",
            "\n",
            "[37] [42, 42, 47, 55, 57] | [42, 42, 42, 55, 57]\n",
            "[464] [3, 16, 16, 18, 55] | [3, 16, 18, 18, 55]\n",
            "[489] [24, 24, 27, 47, 53] | [24, 24, 24, 47, 53]\n",
            "[531] [0, 0, 2, 28, 55] | [0, 2, 2, 28, 55]\n",
            "[597] [5, 5, 6, 13, 22] | [5, 6, 6, 13, 22]\n",
            "[693] [17, 31, 31, 36, 59] | [17, 31, 36, 36, 59]\n",
            "[852] [24, 24, 29, 45, 59] | [24, 24, 24, 45, 59]\n",
            "[891] [30, 30, 35, 37, 46] | [30, 35, 35, 37, 46]\n",
            "[911] [24, 48, 48, 49, 62] | [24, 48, 48, 48, 62]\n",
            "[980] [21, 39, 47, 47, 47] | [21, 47, 47, 47, 47]\n",
            "10/1000 (1.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "other_tests = next(train_gen)\n",
        "predictions = model(other_tests).argmax(dim=-1)[:,LIST_LENGTH+1:-1]"
      ],
      "metadata": {
        "id": "KpFNjBmXkIPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in zip(other_tests, predictions):\n",
        "  incorrect = (x[LIST_LENGTH+2:] != y).prod()\n",
        "  print(f\"{x} | {y}{' Incorrect!' if incorrect else ''}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfXeWxqhkd-q",
        "outputId": "4cfc751d-0f5c-4fcb-dcfe-c64189a5ac0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([64, 39, 37, 15, 10, 46, 65, 10, 15, 37, 39, 46]) | tensor([10, 15, 37, 39, 46])\n",
            "tensor([64, 51, 27, 55,  5, 52, 65,  5, 27, 51, 52, 55]) | tensor([ 5, 27, 51, 52, 55])\n",
            "tensor([64,  8, 51, 40, 47, 18, 65,  8, 18, 40, 47, 51]) | tensor([ 8, 18, 40, 47, 51])\n",
            "tensor([64, 18, 48,  5, 39,  0, 65,  0,  5, 18, 39, 48]) | tensor([ 0,  5, 18, 39, 48])\n",
            "tensor([64, 27, 10, 47,  6, 27, 65,  6, 10, 27, 27, 47]) | tensor([ 6, 10, 27, 27, 47])\n",
            "tensor([64, 14, 58, 35, 40, 35, 65, 14, 35, 35, 40, 58]) | tensor([14, 35, 35, 40, 58])\n",
            "tensor([64, 20, 12, 25, 44, 25, 65, 12, 20, 25, 25, 44]) | tensor([12, 20, 25, 25, 44])\n",
            "tensor([64,  2,  0, 46, 45, 13, 65,  0,  2, 13, 45, 46]) | tensor([ 0,  2, 13, 45, 46])\n",
            "tensor([64, 42, 44,  0, 23, 28, 65,  0, 23, 28, 42, 44]) | tensor([ 0, 23, 28, 42, 44])\n",
            "tensor([64, 22, 47, 19, 32, 55, 65, 19, 22, 32, 47, 55]) | tensor([19, 22, 32, 47, 55])\n",
            "tensor([64, 54, 57, 62, 35, 23, 65, 23, 35, 54, 57, 62]) | tensor([23, 35, 54, 57, 62])\n",
            "tensor([64, 26, 42,  2,  0, 33, 65,  0,  2, 26, 33, 42]) | tensor([ 0,  2, 26, 33, 42])\n",
            "tensor([64, 37,  1, 37, 26, 13, 65,  1, 13, 26, 37, 37]) | tensor([ 1, 13, 26, 37, 37])\n",
            "tensor([64, 22, 60, 62, 55, 22, 65, 22, 22, 55, 60, 62]) | tensor([22, 22, 55, 60, 62])\n",
            "tensor([64, 31, 45, 27,  8, 10, 65,  8, 10, 27, 31, 45]) | tensor([ 8, 10, 27, 31, 45])\n",
            "tensor([64, 54, 19, 43,  1, 30, 65,  1, 19, 30, 43, 54]) | tensor([ 1, 19, 30, 43, 54])\n",
            "tensor([64, 57, 46, 49, 44, 41, 65, 41, 44, 46, 49, 57]) | tensor([41, 44, 46, 49, 57])\n",
            "tensor([64, 55, 39,  6, 13, 35, 65,  6, 13, 35, 39, 55]) | tensor([ 6, 13, 35, 39, 55])\n",
            "tensor([64, 10,  4, 60, 21, 28, 65,  4, 10, 21, 28, 60]) | tensor([ 4, 10, 21, 28, 60])\n",
            "tensor([64, 52,  1, 16, 20, 59, 65,  1, 16, 20, 52, 59]) | tensor([ 1, 16, 20, 52, 59])\n",
            "tensor([64, 34, 23, 18, 22, 59, 65, 18, 22, 23, 34, 59]) | tensor([18, 22, 23, 34, 59])\n",
            "tensor([64, 13,  2, 30, 25,  4, 65,  2,  4, 13, 25, 30]) | tensor([ 2,  4, 13, 25, 30])\n",
            "tensor([64, 19, 26, 16, 16,  1, 65,  1, 16, 16, 19, 26]) | tensor([ 1, 16, 19, 19, 26])\n",
            "tensor([64, 26, 39, 29, 39, 28, 65, 26, 28, 29, 39, 39]) | tensor([26, 28, 29, 39, 39])\n",
            "tensor([64, 15, 37, 57, 18, 33, 65, 15, 18, 33, 37, 57]) | tensor([15, 18, 33, 37, 57])\n",
            "tensor([64, 59, 59, 18, 42, 59, 65, 18, 42, 59, 59, 59]) | tensor([18, 42, 59, 59, 59])\n",
            "tensor([64, 31, 25, 32,  8, 46, 65,  8, 25, 31, 32, 46]) | tensor([ 8, 25, 31, 32, 46])\n",
            "tensor([64, 39, 41,  4, 26, 53, 65,  4, 26, 39, 41, 53]) | tensor([ 4, 26, 39, 41, 53])\n",
            "tensor([64, 60,  2, 39, 12, 30, 65,  2, 12, 30, 39, 60]) | tensor([ 2, 12, 30, 39, 60])\n",
            "tensor([64, 58, 16, 10, 45, 44, 65, 10, 16, 44, 45, 58]) | tensor([10, 16, 44, 45, 58])\n",
            "tensor([64, 25, 62, 29, 58, 51, 65, 25, 29, 51, 58, 62]) | tensor([25, 29, 51, 58, 62])\n",
            "tensor([64, 60, 14, 21, 54, 55, 65, 14, 21, 54, 55, 60]) | tensor([14, 21, 54, 55, 60])\n",
            "tensor([64, 18, 11, 29, 11,  5, 65,  5, 11, 11, 18, 29]) | tensor([ 5, 11, 11, 18, 29])\n",
            "tensor([64, 24, 46,  7,  5, 35, 65,  5,  7, 24, 35, 46]) | tensor([ 5,  7, 24, 35, 46])\n",
            "tensor([64, 18, 32, 52,  1, 20, 65,  1, 18, 20, 32, 52]) | tensor([ 1, 18, 20, 32, 52])\n",
            "tensor([64, 47,  9, 35, 12, 45, 65,  9, 12, 35, 45, 47]) | tensor([ 9, 12, 35, 45, 47])\n",
            "tensor([64, 47, 62, 40,  9,  6, 65,  6,  9, 40, 47, 62]) | tensor([ 6,  9, 40, 47, 62])\n",
            "tensor([64, 60, 31,  6, 52, 43, 65,  6, 31, 43, 52, 60]) | tensor([ 6, 31, 43, 52, 60])\n",
            "tensor([64, 42, 13, 18, 21, 59, 65, 13, 18, 21, 42, 59]) | tensor([13, 18, 21, 42, 59])\n",
            "tensor([64, 32, 32, 59,  0, 28, 65,  0, 28, 32, 32, 59]) | tensor([ 0, 28, 32, 32, 59])\n",
            "tensor([64, 63, 48, 10, 51, 27, 65, 10, 27, 48, 51, 63]) | tensor([10, 27, 48, 51, 63])\n",
            "tensor([64, 35,  5,  6, 46,  8, 65,  5,  6,  8, 35, 46]) | tensor([ 5,  6,  8, 35, 46])\n",
            "tensor([64, 23, 61, 53, 58, 34, 65, 23, 34, 53, 58, 61]) | tensor([23, 34, 53, 58, 61])\n",
            "tensor([64,  0, 29, 14, 48, 18, 65,  0, 14, 18, 29, 48]) | tensor([ 0, 14, 18, 29, 48])\n",
            "tensor([64, 14, 29,  8, 20,  5, 65,  5,  8, 14, 20, 29]) | tensor([ 5,  8, 14, 20, 29])\n",
            "tensor([64, 11, 62,  1,  3, 13, 65,  1,  3, 11, 13, 62]) | tensor([ 1,  3, 11, 13, 62])\n",
            "tensor([64, 62, 31, 40, 28, 18, 65, 18, 28, 31, 40, 62]) | tensor([18, 28, 31, 40, 62])\n",
            "tensor([64,  1, 14, 39, 52, 46, 65,  1, 14, 39, 46, 52]) | tensor([ 1, 14, 39, 46, 52])\n",
            "tensor([64,  2, 54, 44,  5, 50, 65,  2,  5, 44, 50, 54]) | tensor([ 2,  5, 44, 50, 54])\n",
            "tensor([64, 34,  8,  0, 63, 32, 65,  0,  8, 32, 34, 63]) | tensor([ 0,  8, 32, 34, 63])\n",
            "tensor([64, 30, 41, 31, 47, 59, 65, 30, 31, 41, 47, 59]) | tensor([30, 31, 41, 47, 59])\n",
            "tensor([64, 10, 16, 40, 62, 39, 65, 10, 16, 39, 40, 62]) | tensor([10, 16, 39, 40, 62])\n",
            "tensor([64,  8, 61, 16, 58,  2, 65,  2,  8, 16, 58, 61]) | tensor([ 2,  8, 16, 58, 61])\n",
            "tensor([64,  5, 47, 43, 33, 61, 65,  5, 33, 43, 47, 61]) | tensor([ 5, 33, 43, 47, 61])\n",
            "tensor([64, 47, 27, 27, 31,  3, 65,  3, 27, 27, 31, 47]) | tensor([ 3, 27, 27, 31, 47])\n",
            "tensor([64,  3, 29, 58, 54, 52, 65,  3, 29, 52, 54, 58]) | tensor([ 3, 29, 52, 54, 58])\n",
            "tensor([64, 45, 33,  7, 38, 42, 65,  7, 33, 38, 42, 45]) | tensor([ 7, 33, 38, 42, 45])\n",
            "tensor([64, 32, 53, 45, 39, 41, 65, 32, 39, 41, 45, 53]) | tensor([32, 39, 41, 45, 53])\n",
            "tensor([64, 39,  3, 33, 42, 39, 65,  3, 33, 39, 39, 42]) | tensor([ 3, 33, 39, 39, 42])\n",
            "tensor([64, 50, 45, 14, 47, 10, 65, 10, 14, 45, 47, 50]) | tensor([10, 14, 45, 47, 50])\n",
            "tensor([64, 33,  9, 38, 55, 40, 65,  9, 33, 38, 40, 55]) | tensor([ 9, 33, 38, 40, 55])\n",
            "tensor([64, 31, 41, 18, 22,  4, 65,  4, 18, 22, 31, 41]) | tensor([ 4, 18, 22, 31, 41])\n",
            "tensor([64, 22, 22, 55, 59, 48, 65, 22, 22, 48, 55, 59]) | tensor([22, 22, 48, 55, 59])\n",
            "tensor([64, 13, 55, 38, 22, 46, 65, 13, 22, 38, 46, 55]) | tensor([13, 22, 38, 46, 55])\n",
            "tensor([64, 37, 44,  3, 56, 10, 65,  3, 10, 37, 44, 56]) | tensor([ 3, 10, 37, 44, 56])\n",
            "tensor([64,  9, 59,  8, 49, 61, 65,  8,  9, 49, 59, 61]) | tensor([ 8,  9, 49, 59, 61])\n",
            "tensor([64,  2,  3, 50, 46, 26, 65,  2,  3, 26, 46, 50]) | tensor([ 2,  3, 26, 46, 50])\n",
            "tensor([64,  7, 35, 13, 53, 50, 65,  7, 13, 35, 50, 53]) | tensor([ 7, 13, 35, 50, 53])\n",
            "tensor([64, 44,  1, 25,  0, 50, 65,  0,  1, 25, 44, 50]) | tensor([ 0,  1, 25, 44, 50])\n",
            "tensor([64, 45, 36, 44, 18, 53, 65, 18, 36, 44, 45, 53]) | tensor([18, 36, 44, 45, 53])\n",
            "tensor([64, 26, 61, 56, 20, 53, 65, 20, 26, 53, 56, 61]) | tensor([20, 26, 53, 56, 61])\n",
            "tensor([64, 22,  1, 41,  2, 22, 65,  1,  2, 22, 22, 41]) | tensor([ 1,  2, 22, 22, 41])\n",
            "tensor([64, 11, 39, 48, 47, 22, 65, 11, 22, 39, 47, 48]) | tensor([11, 22, 39, 47, 48])\n",
            "tensor([64, 15, 33, 22, 14, 47, 65, 14, 15, 22, 33, 47]) | tensor([14, 15, 22, 33, 47])\n",
            "tensor([64, 19, 30, 59, 31, 23, 65, 19, 23, 30, 31, 59]) | tensor([19, 23, 30, 31, 59])\n",
            "tensor([64,  9, 21, 38, 20,  5, 65,  5,  9, 20, 21, 38]) | tensor([ 5,  9, 20, 21, 38])\n",
            "tensor([64, 63, 42, 19, 25, 49, 65, 19, 25, 42, 49, 63]) | tensor([19, 25, 42, 49, 63])\n",
            "tensor([64, 56,  6, 18, 39, 43, 65,  6, 18, 39, 43, 56]) | tensor([ 6, 18, 39, 43, 56])\n",
            "tensor([64, 55, 18,  7, 52,  7, 65,  7,  7, 18, 52, 55]) | tensor([ 7,  7, 18, 52, 55])\n",
            "tensor([64, 59, 57, 35, 48,  0, 65,  0, 35, 48, 57, 59]) | tensor([ 0, 35, 48, 57, 59])\n",
            "tensor([64, 38, 41, 27,  8, 35, 65,  8, 27, 35, 38, 41]) | tensor([ 8, 27, 35, 38, 41])\n",
            "tensor([64, 36, 53, 44, 16, 30, 65, 16, 30, 36, 44, 53]) | tensor([16, 30, 36, 44, 53])\n",
            "tensor([64, 35, 55, 23, 40, 18, 65, 18, 23, 35, 40, 55]) | tensor([18, 23, 35, 40, 55])\n",
            "tensor([64, 42, 47, 34,  4, 13, 65,  4, 13, 34, 42, 47]) | tensor([ 4, 13, 34, 42, 47])\n",
            "tensor([64, 16, 60,  3,  5, 17, 65,  3,  5, 16, 17, 60]) | tensor([ 3,  5, 16, 17, 60])\n",
            "tensor([64, 10,  2, 63, 40, 11, 65,  2, 10, 11, 40, 63]) | tensor([ 2, 10, 11, 40, 63])\n",
            "tensor([64, 54, 15,  7, 22, 13, 65,  7, 13, 15, 22, 54]) | tensor([ 7, 13, 15, 22, 54])\n",
            "tensor([64, 40, 49, 43, 52,  5, 65,  5, 40, 43, 49, 52]) | tensor([ 5, 40, 43, 49, 52])\n",
            "tensor([64, 47, 30, 28, 37, 46, 65, 28, 30, 37, 46, 47]) | tensor([28, 30, 37, 46, 47])\n",
            "tensor([64, 15, 14, 52, 23, 57, 65, 14, 15, 23, 52, 57]) | tensor([14, 15, 23, 52, 57])\n",
            "tensor([64,  0, 18,  4, 30,  3, 65,  0,  3,  4, 18, 30]) | tensor([ 0,  3,  4, 18, 30])\n",
            "tensor([64, 12, 44,  0, 44, 54, 65,  0, 12, 44, 44, 54]) | tensor([ 0, 12, 44, 44, 54])\n",
            "tensor([64, 27, 44, 43, 45, 53, 65, 27, 43, 44, 45, 53]) | tensor([27, 43, 44, 45, 53])\n",
            "tensor([64, 19, 58, 39, 10, 54, 65, 10, 19, 39, 54, 58]) | tensor([10, 19, 39, 54, 58])\n",
            "tensor([64, 60,  6, 13, 37,  0, 65,  0,  6, 13, 37, 60]) | tensor([ 0,  6, 13, 37, 60])\n",
            "tensor([64, 63, 46, 29, 25, 61, 65, 25, 29, 46, 61, 63]) | tensor([25, 29, 46, 61, 63])\n",
            "tensor([64,  1, 49, 55, 51, 29, 65,  1, 29, 49, 51, 55]) | tensor([ 1, 29, 49, 51, 55])\n",
            "tensor([64, 35, 25, 44, 53, 10, 65, 10, 25, 35, 44, 53]) | tensor([10, 25, 35, 44, 53])\n",
            "tensor([64, 18, 21, 40, 27, 59, 65, 18, 21, 27, 40, 59]) | tensor([18, 21, 27, 40, 59])\n",
            "tensor([64, 55, 24, 52, 33,  2, 65,  2, 24, 33, 52, 55]) | tensor([ 2, 24, 33, 52, 55])\n",
            "tensor([64, 54, 42, 12, 34, 24, 65, 12, 24, 34, 42, 54]) | tensor([12, 24, 34, 42, 54])\n",
            "tensor([64, 51,  7, 24, 52, 34, 65,  7, 24, 34, 51, 52]) | tensor([ 7, 24, 34, 51, 52])\n",
            "tensor([64, 10, 31, 43, 60, 56, 65, 10, 31, 43, 56, 60]) | tensor([10, 31, 43, 56, 60])\n",
            "tensor([64, 40, 13, 28, 22, 25, 65, 13, 22, 25, 28, 40]) | tensor([13, 22, 25, 28, 40])\n",
            "tensor([64, 17, 50, 53, 47, 58, 65, 17, 47, 50, 53, 58]) | tensor([17, 47, 50, 53, 58])\n",
            "tensor([64, 45, 51, 10, 47, 61, 65, 10, 45, 47, 51, 61]) | tensor([10, 45, 47, 51, 61])\n",
            "tensor([64, 34, 32,  8,  7, 28, 65,  7,  8, 28, 32, 34]) | tensor([ 7,  8, 28, 32, 34])\n",
            "tensor([64, 21, 45, 28, 42, 47, 65, 21, 28, 42, 45, 47]) | tensor([21, 28, 42, 45, 47])\n",
            "tensor([64, 34, 15,  2, 39, 11, 65,  2, 11, 15, 34, 39]) | tensor([ 2, 11, 15, 34, 39])\n",
            "tensor([64,  0, 15, 32, 59, 29, 65,  0, 15, 29, 32, 59]) | tensor([ 0, 15, 29, 32, 59])\n",
            "tensor([64, 18, 30,  9, 54, 43, 65,  9, 18, 30, 43, 54]) | tensor([ 9, 18, 30, 43, 54])\n",
            "tensor([64,  9, 32, 24, 59, 20, 65,  9, 20, 24, 32, 59]) | tensor([ 9, 20, 24, 32, 59])\n",
            "tensor([64, 47, 21, 47, 51, 55, 65, 21, 47, 47, 51, 55]) | tensor([21, 47, 47, 51, 55])\n",
            "tensor([64,  1,  8, 51, 32, 31, 65,  1,  8, 31, 32, 51]) | tensor([ 1,  8, 31, 32, 51])\n",
            "tensor([64, 13, 63, 54, 38,  1, 65,  1, 13, 38, 54, 63]) | tensor([ 1, 13, 38, 54, 63])\n",
            "tensor([64, 51, 54, 53, 28, 26, 65, 26, 28, 51, 53, 54]) | tensor([26, 28, 51, 53, 54])\n",
            "tensor([64, 13, 60, 61, 21, 11, 65, 11, 13, 21, 60, 61]) | tensor([11, 13, 21, 60, 61])\n",
            "tensor([64, 49, 45, 44, 13,  2, 65,  2, 13, 44, 45, 49]) | tensor([ 2, 13, 44, 45, 49])\n",
            "tensor([64, 14,  9,  5, 57, 51, 65,  5,  9, 14, 51, 57]) | tensor([ 5,  9, 14, 51, 57])\n",
            "tensor([64, 11, 58, 25, 57, 50, 65, 11, 25, 50, 57, 58]) | tensor([11, 25, 50, 57, 58])\n",
            "tensor([64,  8, 11, 45, 20, 50, 65,  8, 11, 20, 45, 50]) | tensor([ 8, 11, 20, 45, 50])\n",
            "tensor([64,  6, 41, 32, 21, 56, 65,  6, 21, 32, 41, 56]) | tensor([ 6, 21, 32, 41, 56])\n",
            "tensor([64, 13, 40, 35, 44, 52, 65, 13, 35, 40, 44, 52]) | tensor([13, 35, 40, 44, 52])\n",
            "tensor([64, 40, 38, 28, 20, 34, 65, 20, 28, 34, 38, 40]) | tensor([20, 28, 34, 38, 40])\n",
            "tensor([64,  5, 55, 42, 31, 23, 65,  5, 23, 31, 42, 55]) | tensor([ 5, 23, 31, 42, 55])\n",
            "tensor([64, 55, 50, 23, 57,  4, 65,  4, 23, 50, 55, 57]) | tensor([ 4, 23, 50, 55, 57])\n",
            "tensor([64,  6, 61, 20, 55, 39, 65,  6, 20, 39, 55, 61]) | tensor([ 6, 20, 39, 55, 61])\n",
            "tensor([64, 63, 61, 33, 45, 33, 65, 33, 33, 45, 61, 63]) | tensor([33, 33, 45, 61, 63])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving trained model"
      ],
      "metadata": {
        "id": "pNofXtfSQvh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_timestamp() -> str:\n",
        "    return dt.now().isoformat(\"T\", \"minutes\").replace(\":\", \"-\")\n",
        "\n",
        "if not os.path.isdir(\"models\"):\n",
        "    os.mkdir(\"models\")\n",
        "\n",
        "fname = f\"model_state_dict_{make_timestamp()}.pkl\"\n",
        "\n",
        "with open(\"models/\" + fname, \"wb\") as f:\n",
        "    pickle.dump(model.state_dict(), f)"
      ],
      "metadata": {
        "id": "i9lLO9p1d8mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interior of the model"
      ],
      "metadata": {
        "id": "WBmA4marmvzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def observing_test(list_to_order: list[int]):\n",
        "  input = tensor([64] + list_to_order + [65] + sorted(list_to_order))\n",
        "  logits, cache_model = model.run_with_cache(input, remove_batch_dim=True) \n",
        "  preds = logits[:,LIST_LENGTH+1:-1].argmax(-1)[0]\n",
        "  attention_pattern = cache_model[\"pattern\", 0, \"attn\"]\n",
        "  tokens_input = list(map(str,list(input)))\n",
        "  print(str(list_to_order) + \" -> \" + str(sorted(list_to_order)))\n",
        "  print(\"prediction: \", preds.tolist())\n",
        "  print(\"Correct\" if (preds.tolist() == sorted(list_to_order)) else \"Incorrect\")\n",
        "  return tokens_input, attention_pattern"
      ],
      "metadata": {
        "id": "YO1-8HZKqAWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok, att = observing_test([5, 1, 2, 5, 50])\n",
        "cv.attention.attention_patterns(tokens=tok, attention=att)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "FTnl0kYRrzqG",
        "outputId": "400705ce-f718-4450-8dec-43fcc4e57705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 1, 2, 5, 50] -> [1, 2, 5, 5, 50]\n",
            "prediction:  [1, 2, 5, 5, 50]\n",
            "Correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f2a79849460>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-c6f6f597-4f72\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.38.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-c6f6f597-4f72\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"tensor(64)\", \"tensor(5)\", \"tensor(1)\", \"tensor(2)\", \"tensor(5)\", \"tensor(50)\", \"tensor(65)\", \"tensor(1)\", \"tensor(2)\", \"tensor(5)\", \"tensor(5)\", \"tensor(50)\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8382039070129395, 0.16179610788822174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.773255467414856, 0.14419499039649963, 0.08254953473806381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5638338327407837, 0.20928256213665009, 0.13035672903060913, 0.0965268537402153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6262611746788025, 0.07357944548130035, 0.12023221701383591, 0.10541816055774689, 0.07450903207063675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10146607458591461, 0.13826629519462585, 0.08890961855649948, 0.114606112241745, 0.14133712649345398, 0.41541483998298645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007105127442628145, 0.09431827068328857, 0.5149751305580139, 0.29864636063575745, 0.09014517068862915, 0.0005694014253094792, 0.0006351416814140975, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1182774230837822, 0.22066278755664825, 0.11233830451965332, 0.2321786731481552, 0.21666035056114197, 0.04108854755759239, 0.04022401198744774, 0.018570007756352425, 0.0, 0.0, 0.0, 0.0], [0.14591769874095917, 0.2173355221748352, 0.09732086211442947, 0.09929539263248444, 0.21572092175483704, 0.10168793797492981, 0.08198609948158264, 0.015247968956828117, 0.025487588718533516, 0.0, 0.0, 0.0], [0.19498471915721893, 0.1027107685804367, 0.08727859705686569, 0.09168237447738647, 0.10221045464277267, 0.2315419614315033, 0.14766228199005127, 0.009907938539981842, 0.013628869317471981, 0.018391983583569527, 0.0, 0.0], [0.09206033498048782, 0.032383304089307785, 0.03364928066730499, 0.03607965260744095, 0.03226713091135025, 0.5502167344093323, 0.20826761424541473, 0.0043559568002820015, 0.0031243162229657173, 0.0027121177408844233, 0.0048834835179150105, 0.0], [0.01966678909957409, 0.06247618421912193, 0.04660862684249878, 0.05291873589158058, 0.06339141726493835, 0.3128608465194702, 0.29621943831443787, 0.016277389600872993, 0.010404075495898724, 0.007963481359183788, 0.007384142838418484, 0.1038288027048111]]]}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok, att = observing_test([30, 30, 3, 2, 1])\n",
        "cv.attention.attention_patterns(tokens=tok, attention=att)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "TjOgtFPUolbM",
        "outputId": "7d675c11-f05b-43a2-9823-4018081dbb60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30, 30, 3, 2, 1] -> [1, 2, 3, 30, 30]\n",
            "prediction:  [1, 2, 3, 30, 30]\n",
            "Correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f2a798a9250>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-666bbfe0-61d9\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.38.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-666bbfe0-61d9\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"tensor(64)\", \"tensor(30)\", \"tensor(30)\", \"tensor(3)\", \"tensor(2)\", \"tensor(1)\", \"tensor(65)\", \"tensor(1)\", \"tensor(2)\", \"tensor(3)\", \"tensor(30)\", \"tensor(30)\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07447192817926407, 0.9255281090736389, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09292228519916534, 0.455316424369812, 0.4517611861228943, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7083879113197327, 0.09359297901391983, 0.09579221904277802, 0.10222689807415009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.569575846195221, 0.03816564381122589, 0.03890179842710495, 0.2809006869792938, 0.07245602458715439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6046651601791382, 0.010913250967860222, 0.010601673275232315, 0.1855587661266327, 0.1408226042985916, 0.04743852838873863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007013861322775483, 0.002829322125762701, 0.0028275297954678535, 0.1854015737771988, 0.3073415160179138, 0.5002716779708862, 0.0006269826553761959, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13320349156856537, 0.07553618401288986, 0.07460883259773254, 0.25836843252182007, 0.26382115483283997, 0.1282484382390976, 0.04530009627342224, 0.02091345377266407, 0.0, 0.0, 0.0, 0.0], [0.15510746836662292, 0.1352507770061493, 0.13774429261684418, 0.23551803827285767, 0.10444356501102448, 0.10148532688617706, 0.08714950829744339, 0.01620827242732048, 0.02709277532994747, 0.0, 0.0, 0.0], [0.23137980699539185, 0.16495056450366974, 0.167533278465271, 0.09295284003019333, 0.09067345410585403, 0.08197095990180969, 0.12199366092681885, 0.011238438077270985, 0.01709647662937641, 0.020210497081279755, 0.0, 0.0], [0.003112498205155134, 0.27069228887557983, 0.26749110221862793, 0.010839143767952919, 0.011552389711141586, 0.007451901212334633, 0.4183274507522583, 0.00033916367101483047, 0.0001604139688424766, 0.00011364703823346645, 0.009919943287968636, 0.0], [0.018367409706115723, 0.22732113301753998, 0.2285696119070053, 0.0417330302298069, 0.05514904856681824, 0.054067354649305344, 0.17595310509204865, 0.014264988712966442, 0.010625232011079788, 0.008073308505117893, 0.09771119803190231, 0.06816458702087402]]]}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok, att = observing_test([1, 1, 2, 1, 1])\n",
        "cv.attention.attention_patterns(tokens=tok, attention=att)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "1epYfMz5nyHr",
        "outputId": "4bd5c1cf-32b5-48e0-ad36-aec4e81071e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2, 1, 1] -> [1, 1, 1, 1, 2]\n",
            "prediction:  [1, 1, 1, 1, 2]\n",
            "Correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f2a798494f0>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-53f2cbe6-e5c2\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.38.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-53f2cbe6-e5c2\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"tensor(64)\", \"tensor(1)\", \"tensor(1)\", \"tensor(2)\", \"tensor(1)\", \"tensor(1)\", \"tensor(65)\", \"tensor(1)\", \"tensor(1)\", \"tensor(1)\", \"tensor(1)\", \"tensor(2)\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8848952651023865, 0.11510473489761353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8215433955192566, 0.0907520055770874, 0.08770455420017242, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6147310733795166, 0.1379045695066452, 0.1421240270137787, 0.10524032264947891, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6184172034263611, 0.06046104058623314, 0.058149490505456924, 0.2022964507341385, 0.060675833374261856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6475939154624939, 0.051111117005348206, 0.04944844916462898, 0.14967003464698792, 0.051370080560445786, 0.05080647021532059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00030587069340981543, 0.21223042905330658, 0.22169312834739685, 0.12856513261795044, 0.21876612305641174, 0.21816587448120117, 0.0002734239969868213, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13722705841064453, 0.13144280016422272, 0.13033640384674072, 0.26937681436538696, 0.13128095865249634, 0.13212233781814575, 0.046668440103530884, 0.0215451717376709, 0.0, 0.0, 0.0, 0.0], [0.19046977162361145, 0.10696393996477127, 0.10538390278816223, 0.24696654081344604, 0.10633781552314758, 0.10723406076431274, 0.08729401230812073, 0.018218059092760086, 0.03113190457224846, 0.0, 0.0, 0.0], [0.2489963173866272, 0.08018124848604202, 0.07830780744552612, 0.2126697450876236, 0.07928919047117233, 0.08020181953907013, 0.13290022313594818, 0.020594364032149315, 0.028527067974209785, 0.03833219036459923, 0.0, 0.0], [0.22566641867160797, 0.05045109614729881, 0.04813992977142334, 0.18867942690849304, 0.04923884943127632, 0.05012933164834976, 0.32928773760795593, 0.01820654794573784, 0.013322392478585243, 0.013742479495704174, 0.013135726563632488, 0.0], [0.14008665084838867, 0.054327379912137985, 0.05666128918528557, 0.036761291325092316, 0.055329449474811554, 0.05467693507671356, 0.02396843209862709, 0.0982283279299736, 0.12856601178646088, 0.11964790523052216, 0.15073537826538086, 0.08101096749305725]]]}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok, att = observing_test([60, 50, 50, 50, 50])\n",
        "cv.attention.attention_patterns(tokens=tok, attention=att)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "W-lnzPtTojH2",
        "outputId": "5fb99f9e-9d47-465d-fcf7-262c2f6e6c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[60, 50, 50, 50, 50] -> [50, 50, 50, 50, 60]\n",
            "prediction:  [50, 50, 50, 50, 60]\n",
            "Correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f2a79921490>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-8a07fcb0-af45\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.38.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-8a07fcb0-af45\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"tensor(64)\", \"tensor(60)\", \"tensor(50)\", \"tensor(50)\", \"tensor(50)\", \"tensor(50)\", \"tensor(65)\", \"tensor(50)\", \"tensor(50)\", \"tensor(50)\", \"tensor(50)\", \"tensor(60)\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07057645916938782, 0.9294235706329346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0628557950258255, 0.6675413250923157, 0.26960289478302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012198169715702534, 0.6185010671615601, 0.1836027354001999, 0.1856980174779892, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.026784783229231834, 0.3870495855808258, 0.1934388279914856, 0.1935984045267105, 0.19912834465503693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03671861067414284, 0.36209291219711304, 0.14883273839950562, 0.14961768686771393, 0.15240737795829773, 0.15033058822155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18048568069934845, 0.0777367353439331, 0.1478578746318817, 0.1450396329164505, 0.1429000198841095, 0.14464029669761658, 0.16133973002433777, 0.0, 0.0, 0.0, 0.0, 0.0], [0.021457813680171967, 0.21772395074367523, 0.15920203924179077, 0.15931829810142517, 0.16026148200035095, 0.1606995165348053, 0.11501941084861755, 0.0063173603266477585, 0.0, 0.0, 0.0, 0.0], [0.006868528667837381, 0.2852146327495575, 0.15066389739513397, 0.15113112330436707, 0.1526837944984436, 0.15296441316604614, 0.09579518437385559, 0.002266667317599058, 0.0024117666762322187, 0.0, 0.0, 0.0], [0.002352737123146653, 0.34416288137435913, 0.13961435854434967, 0.14048896729946136, 0.14257697761058807, 0.14285777509212494, 0.08415091037750244, 0.0013086271937936544, 0.0011057438096031547, 0.001381074427627027, 0.0, 0.0], [9.081103780772537e-05, 0.5808804035186768, 0.09376659989356995, 0.09667080640792847, 0.09743533283472061, 0.09765049815177917, 0.032033007591962814, 0.0003113945131190121, 0.00013736153778154403, 0.0001505748077761382, 0.000873201759532094, 0.0], [0.020790278911590576, 0.28669244050979614, 0.07685745507478714, 0.08040913939476013, 0.07909495383501053, 0.08162898570299149, 0.09072937071323395, 0.044835466891527176, 0.03200985863804817, 0.02751428447663784, 0.01526977401226759, 0.16416801512241364]]]}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn = model.blocks[0].attn\n",
        "embedding_numbers = model.embed(tensor(list(range(66))))\n",
        "matrix_key = attn.W_K\n",
        "matrix_query = attn.W_Q\n",
        "keys_numbers = embedding_numbers @ matrix_key\n",
        "queries_numbers = embedding_numbers @ matrix_query"
      ],
      "metadata": {
        "id": "1fU0N_Zo0Lv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "attention_between_numbers = torch.einsum(\"ijr,ikr->ijk\", queries_numbers, keys_numbers).detach().numpy()[0]\n",
        "heatmap = plt.imshow(attention_between_numbers, cmap='hot', interpolation='nearest')\n",
        "plt.colorbar(heatmap)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "Itl56nhGw0Lw",
        "outputId": "8cd7ad45-ee65-44a7-8a80-f73ce931543f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_diag = attention_between_numbers.diagonal().mean()\n",
        "mean_absolute_diag = np.absolute(attention_between_numbers.diagonal()).mean()\n",
        "print(\"Mean value on the diagonal: \", mean_diag)\n",
        "print(\"Mean absolute value on the diagonal: \", mean_absolute_diag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g24DG1ttt-by",
        "outputId": "203f303a-da14-44de-f28f-3c0dfa4918fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean value on the diagonal:  0.043144636\n",
            "Mean absolute value on the diagonal:  0.055903777\n"
          ]
        }
      ]
    }
  ]
}